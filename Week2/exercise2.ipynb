{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f9e54c",
   "metadata": {},
   "source": [
    "# Week 2: Summary statistics, similarity, and nearest neighbors  <span style=\"font-size: 0.3em;\">v20260209a</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b727381",
   "metadata": {},
   "source": [
    "**Content:**\n",
    "- Part 1: Summary statistics and measures of similarity\n",
    "- Part 2: Nearest neighbors method in Python\n",
    "- Part 3: Supervised learning on the Wine data\n",
    "- Assignment 2: Similarity and kNN methods on the Whisky dataset\n",
    "\n",
    "**Objectives:**\n",
    "- Understand how to calculate summary statistics such as mean, variance, median, range, covariance and correlation.\n",
    "- Understand the various measures of similarity, such as Jaccard and Cosine similarity and apply similarity measures to query for similar observations.\n",
    "- Become familiar with fitting $k$-nearest neighbor models in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc68d0e",
   "metadata": {},
   "source": [
    "## Commands and Methods Used in the Notebook\n",
    "\n",
    "We will focus on two key libraries used for machine learning, namely NumPy (which we assume you already know) and scikit-learn (sklearn). We will use Pandas and dataframes to load and hold the data for easy access. **scikit-learn (sklearn)** is a popular Python library for machine learning that provides simple, consistent tools for training models and making predictions. In this assignment, it is used to apply standard machine learning algorithms without implementing them from scratch. \n",
    "\n",
    "The exercises use (or ask you to use) the following commands and methods:\n",
    "\n",
    "- `numpy`\n",
    "  - `np.array`: create a NumPy array\n",
    "  - `np.load`: load `.npy` files\n",
    "  - `np.mean`: average values\n",
    "  - `np.std`: standard deviation (with `ddof` parameter for biased/unbiased)\n",
    "  - `np.median`: median values\n",
    "  - `np.min`: minimum value\n",
    "  - `np.max`: maximum value\n",
    "  - `np.sum`: sum of values\n",
    "  - `np.dot`: dot product\n",
    "  - `np.linalg.norm`: vector/matrix norm\n",
    "  - `np.power`: elementwise exponentiation\n",
    "  - `np.argsort`: indices that would sort an array\n",
    "  - `np.unique`: find unique elements\n",
    "  - `np.random.seed`: set random seed\n",
    "  - `np.random.rand`: random values\n",
    "  - `np.meshgrid`: create coordinate matrices\n",
    "  - `np.corrcoef`: computes correlation matrix\n",
    "\n",
    "- `pandas` (for loading and holding data, not for actual processing or analysis)\n",
    "  - `pd.read_csv`: load CSV data\n",
    "\n",
    "- `matplotlib.pyplot`\n",
    "  - `plt.subplots`: create figures and axes\n",
    "  - `plt.imshow`: show a matrix as an image\n",
    "  - `plt.show`: display plots\n",
    "  - `plt.xticks`: set x-axis tick labels\n",
    "  - `plt.yticks`: set y-axis tick labels\n",
    "\n",
    "- `seaborn`\n",
    "  - `sns.set_style`: set plot style\n",
    "  - `sns.set_theme`: set plot theme defaults\n",
    "\n",
    "- `sklearn.metrics`\n",
    "  - `confusion_matrix`: compute confusion matrix\n",
    "  - `jaccard_score`: Jaccard similarity\n",
    "  - `pairwise.cosine_similarity`: cosine similarity\n",
    "\n",
    "- `sklearn.neighbors`\n",
    "  - `KNeighborsClassifier`: k-nearest neighbors classifier\n",
    "    - `.fit()`: train the classifier\n",
    "    - `.predict()`: make predictions\n",
    "\n",
    "- `sklearn.model_selection`\n",
    "  - `train_test_split`: split data into train/test sets\n",
    "\n",
    "- `scipy.stats`\n",
    "  - `pearsonr`: Pearson correlation coefficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51312853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_theme(font_scale=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758878e5",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this exercise, we start by taking a closer look on some of the building blocks of machine learning and statistical modeling, namely **summary statistics** and **similarity measures**. While summary statistics capture key properties of data and help us understand and compare datasets, similarity metrics such as cosine similarity and norm-based distances are essential for certain machine learning methods, e.g. **$k$-nearest neighbors (kNN)** (This week) and **decision trees (DCTs)** (Week6). In the final parts of the exercise, we will do **supervised learning** with kNNs on real datasets.\n",
    "\n",
    "We first recap a few basic summary statistics and make sure you are comfortable computing these by hand and in Python. You may need to look up the definitions in the lecture notes.\n",
    "\n",
    "**Task I.1:** Using pen and paper and a **basic** electronic calculator (e.g. on your computer), calculate the (empirical) mean, standard deviation (unbiased), median, and range of the following collection of numbers:\n",
    "> $\\mathbf{x}=[-0.68, -2.11,  2.39,  0.26,  1.46,  1.33,  1.03, -0.41, -0.33, 0.47]$\n",
    "\n",
    "**Task I.2:** Verify your results by computing the same quantities with Python.\n",
    "> *Hint:* Look at the help page of the functions `np.mean()`, `np.std()`, `np.median()`, `np.min()` and `np.max()`.\n",
    "\n",
    "> *Hint:* You need to use 1 degree of freedom for the standard deviation - i.e. `std(..., ddof=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aef2f6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea0c4d4bd00adcd2ae75774c00d7106d",
     "grade": false,
     "grade_id": "cell-c8550f2b75920985",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([-0.68, -2.11, 2.39, 0.26, 1.46, 1.33, 1.03, -0.41, -0.33, 0.47])\n",
    "\n",
    "# I.2: Compute values, save in variables mean_x, std_x, median_x, range_x\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Display results\n",
    "print(\"Vector:\", x)\n",
    "print(\"Mean:\", mean_x)\n",
    "print(\"Standard Deviation:\", std_x)\n",
    "print(\"Median:\", median_x)\n",
    "print(\"Range:\", range_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beea8df5",
   "metadata": {},
   "source": [
    "**Task I.3:** Compute both the unbiased and biased estimate of the standard deviation using `np.std()`. Are the two estimates the same? What is the difference?\n",
    "> *Hint:* Consider what the argument `ddof=1` means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec23a62",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1de5c2fe124a452d61295a622edf2db",
     "grade": false,
     "grade_id": "cell-71619e9b03928965",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# I.3: Compute biased and unbiased estimate of the standard deviation, save as std_x_biased and std_x_unbiased\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Display results\n",
    "print(\"Biased Standard Deviation:\", std_x_biased)\n",
    "print(\"Unbiased Standard Deviation:\", std_x_unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21ed83b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 1: Using similarity measures to retrieve information\n",
    "\n",
    "We will take a closer look at **similarity measures** and how they form the basis of information retrieval systems. Specifically, we examine how to extract the most similar items from a **database** given a **query** - a central mechanism in **recommender systems** that is used in everything from movie suggestions on Netflix to improved response generation in large language models (LLMs) such as ChatGPT. \n",
    "\n",
    "Specifically, we have a dataset of $N=9298$ handwritten digits (USPS handwritten digits database). <br>The images are of shape $16 \\times 16$ pixels - the images have been \"flattened\" to store them as vectors, of size $M = 16\\times 16 = 256$. \n",
    "\n",
    "The dataset therefore forms a matrix of size $(N,M) = (9298, 256)$.\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{X} = \\begin{bmatrix} \\boldsymbol{x}_1 \\\\ \\boldsymbol{x}_2 \\\\ \\vdots \\\\ \\boldsymbol{x}_N \\end{bmatrix} = \\underbrace{ \\begin{bmatrix} x_{1,1} & x_{1,2} & \\cdots & x_{1,256} \\\\ x_{2,1} & x_{2,2} & \\cdots & x_{2,256} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{N,1} & x_{N,2} & \\cdots & x_{N,256} \\end{bmatrix}}_{M}\n",
    "$$\n",
    "Hence, each attribute corresponds to a particular pixel in the image where its value defines the grayscale intensity of the pixel.\n",
    "\n",
    "The goal is to match a **query image**, $\\boldsymbol{q}$, to the images in the database $\\boldsymbol{X}$, and extract the most similar one(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ca108",
   "metadata": {},
   "source": [
    "**Task 1.0:** Make sure you understand what the rows and columns of the above matrix correspond to before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23bb102",
   "metadata": {},
   "source": [
    "**Task 1.1:** Check the associated data folder. Load the `digits.npy` file. \n",
    "\n",
    "> *Hint:* You can load `.npy`-files using `np.load(FILE_PATH)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640735b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a4ea412230a2fdae752484ca261ad35",
     "grade": false,
     "grade_id": "cell-4b2e2dc5a9886f52",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 1.1: Load the digits dataset with np.load, save as \"digits\". \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"Shape of digits dataset:\", digits.shape)\n",
    "print(\"First sample:\", digits[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699497fc",
   "metadata": {},
   "source": [
    "As described, we will extract a datapoint from the dataset, as our **query image**, $\\boldsymbol{q}$.\n",
    "\n",
    "**Task 1.2:** Extract the first image and define it as the query vector, i.e. $\\boldsymbol{q} = \\boldsymbol{x}_1$. Make sure to remove the query image from the data matrix $\\boldsymbol{X}$ accordingly.\n",
    "\n",
    "> *Hint:* If in doubt, look up \"NumPy Array Indexing\".\n",
    "\n",
    "> *Hint:* Make sure the query vector is of shape `(1, 256)` - for help, see `.reshape()` method. <br>You can use the `.shape` attribute to check the current shape of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d742267b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7154fc36f8b60976b28da8908d32809c",
     "grade": false,
     "grade_id": "cell-f96d429796376752",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 1.2: Split the digits dataset into a query, q, and a set of images, X.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Check the shape of the digits dataset\n",
    "assert X.shape == (9297, 16*16), \"There should be 9298 samples and 16x16 = 256 features\"\n",
    "assert q.shape == (1, 16*16), \"There should be 1 sample and 16x16 = 256 features\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e72b2bf",
   "metadata": {},
   "source": [
    "We will now plot the query image, $\\boldsymbol{q}$ and the first 4 images of $\\boldsymbol{X}$ for you.\n",
    "\n",
    "> *Hint:* Notice how we need to reshape the flattened image-vectors into matrices of size (16, 16) before plotting them, to make them look like images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946841ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of images to plot\n",
    "plot_n_images = 5\n",
    "\n",
    "# Setup figure\n",
    "fig, axs = plt.subplots(1, plot_n_images, figsize=(4*plot_n_images,4))\n",
    "\n",
    "# Plot the query image\n",
    "axs[0].imshow(q.reshape(16, 16), cmap='gray')\n",
    "axs[0].set_title(r'$\\boldsymbol{q}$')\n",
    "axs[0].axis('off')\n",
    "\n",
    "# Plot the other images\n",
    "for i in range(plot_n_images - 1):\n",
    "    # Extract image number i (since numpy arrays are 0-indexed)\n",
    "    digit = X[i].reshape(16, 16)  # Reshape to 16x16\n",
    "    # Show image of digit\n",
    "    axs[i+1].imshow(digit, cmap='gray')\n",
    "    # Set title\n",
    "    axs[i+1].set_title(rf\"$\\boldsymbol{{x}}_{{{i+1}}}$\")\n",
    "    # Remove axes\n",
    "    axs[i+1].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74da0994",
   "metadata": {},
   "source": [
    "To measure similarity between $\\boldsymbol{q}$ and the elements in $\\boldsymbol{X}$, we will consider the measures SMC, Jaccard, Cosine, ExtendedJaccard, and Correlation, given by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\mathrm{SMC}(\\boldsymbol{q},\\boldsymbol{x}_i)&=\\frac{\\text{Number of matching attribute values}}{\\text{Number of attributes}} = \\frac{\\sum_{j=1}^M \\mathbf{1}\\left[x_{i,j} = q_j\\right]}{M}\\\\ \\\\\n",
    "  \\mathrm{Jaccard}(\\boldsymbol{q},\\boldsymbol{x}_i)&=\\frac{\\text{Number of 11 matching attributes}}{\\text{Number of attributes not involved in 00 matches}} = \\frac{\\sum_{j=1}^M \\mathbf{1}\\left[x_{i,j} = 1, q_{j} = 1\\right]}{M - \\sum_{j=1}^M  \\mathbf{1}\\left[x_{i,j} = 0, q_{j} = 0\\right]  } \n",
    "\\end{align*}\n",
    "$$\n",
    "$$\n",
    "  \\mathrm{Cosine}(\\boldsymbol{q},\\boldsymbol{x}_i) = \\frac{\\boldsymbol{q}^\\top \\boldsymbol{x}_i}{\\|\\boldsymbol{q}\\|\\|\\boldsymbol{x}_i\\|}, \\quad \\qquad\n",
    "  \\mathrm{ExtendedJaccard}(\\boldsymbol{q},\\boldsymbol{x}_i) = \\frac{\\boldsymbol{q}^\\top \\boldsymbol{x}_i}{\\|\\boldsymbol{q}\\|^2+\\|\\boldsymbol{x}_i\\|^2-\\boldsymbol{q}^\\top \\boldsymbol{x}_i}, \\qquad \\quad\n",
    "  \\mathrm{Correlation}(\\boldsymbol{q},\\boldsymbol{x}_i) = \\frac{\\text{cov}(\\boldsymbol{q},\\boldsymbol{x}_i)}{\\text{std}(\\boldsymbol{q})\\text{std}(\\boldsymbol{x}_i)}\n",
    "$$\n",
    "where $\\mathbf{1}\\left[x_{i,j} = q_j\\right]$ denotes the indicator function, $\\displaystyle\\mathrm{cov}(\\boldsymbol{x},\\boldsymbol{y})$ denotes the covariance between $\\boldsymbol{x}$ and $\\boldsymbol{y}$ and $\\text{std}(\\boldsymbol{x})$ denotes the standard deviation of $\\boldsymbol{x}$.\n",
    "\n",
    "The SMC and Jaccard similarity measures are only defined for binary data, i.e., data that takes values of $\\{0,1\\}$. As the\n",
    "data we analyze is non-binary *(Notice how some of the pixels are not completely black or white?)*, we will transform the data to be binary when calculating these two measures of similarity by thresholding according to the median per-pixel intensity, i.e. setting\n",
    "$$\n",
    "  x_{i,j}=\\left\\{\\begin{array}{ll} 0 & \\mathrm{if}\\ x_{i, j}<\\mathrm{median}(\\boldsymbol{X}_{:, j}) \\\\\n",
    "    1 & \\mathrm{otherwise}\\end{array} \\right.\n",
    "$$\n",
    "*Note that, depending on the situation, it can be incorrect to encode information in a single binary attribute - and this is true for binary attributes in general. If the meaning behind the value 0 is not specifically non-presence of an attribute, it can be erroneous. For instance, if male/female is encoded in one binary attribute (male: 0, female: 1), some measures will not model the information carried in being male, and a one-of-out-K encoding would be a proper representation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26621a34",
   "metadata": {},
   "source": [
    "**Task 1.3:** Compute the median per-pixel intensity for the images in $\\boldsymbol{X}$, save as `median_per_pixel`. \n",
    "\n",
    "> *Hint:* What should you set the `axis` argument to when using `np.median()`? <br> Do we want the median of all pixels per image, or the median of each pixel across all images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedf746",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "149fc1b7fcac5b3eaf94217eeba6a39c",
     "grade": false,
     "grade_id": "cell-fcae70709472bcbf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 1.3: Compute the median per pixel\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "assert median_per_pixel.shape == (256,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f3de3d",
   "metadata": {},
   "source": [
    "**Task 1.4:** Binarize the images and the query using the above criteria, save them as `X_binarized` and `q_binarized`.\n",
    "\n",
    "> *Hint:* You can construct a boolean mask using the `>` operator to compare the images with the median per pixel. This will create a list of True and False, stating whether each pixel is above the threshold or not. \n",
    "\n",
    "> *Hint:* Then use `.astype(int)` to convert the boolean arrays to binary (ones and zeros) image arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e98e42",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d0f6cd142a76ecc11591327fd0f7155",
     "grade": false,
     "grade_id": "cell-bf2dc7db094d92ba",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 1.4: Binarize the images, save as X_binarized and q_binarized\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Check if all values are binary\n",
    "assert all(np.unique(X_binarized) == [0, 1]), \"X_binarized should be binary...\"\n",
    "assert all(np.unique(q_binarized) == [0, 1]), \"q_binarized should be binary\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476fd029",
   "metadata": {},
   "source": [
    "We now plot the first 5 samples (when including the query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20,4))\n",
    "\n",
    "# Plot the query image\n",
    "axs[0].imshow(q_binarized.reshape(16, 16), cmap='gray')\n",
    "axs[0].set_title(r'$\\boldsymbol{q}$')\n",
    "axs[0].axis('off')\n",
    "\n",
    "for i in range(plot_n_images - 1):\n",
    "    # Extract image number i\n",
    "    digit = X_binarized[i].reshape(16, 16)  # Reshape to 16x16\n",
    "    # Show image of digit\n",
    "    axs[i+1].imshow(digit, cmap='gray')\n",
    "    # Set title\n",
    "    axs[i+1].set_title(rf\"$\\boldsymbol{{x}}_{i}$\")\n",
    "    # Remove axes\n",
    "    axs[i+1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f76a0",
   "metadata": {},
   "source": [
    "**Task 1.5:** Implement at least two of the similarity measures presented above as python functions. Use them to compute the similarity between $\\boldsymbol{q}_{\\text{binarized}}$ and each image in $\\boldsymbol{X}_{\\text{binarized}}$.\n",
    "\n",
    "> *Hint:* The functions `np.sum()`, `np.dot()`, `np.linalg.norm()` and `np.power()` might be useful.\n",
    "\n",
    "> *Hint:* Give it an honest try! But if it's really difficult, check out `sklearn.metrics.pairwise.cosine_similarity`, `sklearn.metrics.jaccard_score` or `scipy.stats.pearsonr`.\n",
    "\n",
    "> *Hint:* Make sure to remove any similarity functions that you have not implemented from the `similarity_functions` list at the bottom in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50d6c45",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41855dc70887d7cd3f0dcde59fbc72e1",
     "grade": false,
     "grade_id": "cell-ba6670c937610592",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 1.5: Implement at least 2 of the following similarity functions.\n",
    "\n",
    "def smc(q, X):\n",
    "    # q: the binarized query vector of shape (1 x M)\n",
    "    # X: a set of binarized database vectors of shape (N x M)\n",
    "    M = X.shape[1]\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "def jaccard(q, X): # or alternatively using sklearn.metrics.jaccard_score\n",
    "    # q: the binarized query vector of shape (1 x M)\n",
    "    # X: a set of binarized database vectors of shape (N x M)\n",
    "    M = X.shape[1]\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return similarity_score\n",
    "\n",
    "def cosine(q, X):\n",
    "    # q: the query vector of shape (1 x M)\n",
    "    # X: a set of database vectors of shape (N x M)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return similarity_score\n",
    "\n",
    "def extended_jaccard(q, X):\n",
    "    # q: the query vector of shape (1 x M)\n",
    "    # X: a set of database vectors of shape (N x M)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return similarity_score\n",
    "\n",
    "def correlation(q, X):\n",
    "    # q: the query vector of shape (1 x M)\n",
    "    # X: a set of database vectors of shape (N x M)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return similarity_score\n",
    "\n",
    "# Remove any similarity-functions that you have not implemented from the list below:\n",
    "similarity_functions = [smc, jaccard, cosine, extended_jaccard, correlation]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977407fc",
   "metadata": {},
   "source": [
    "We provide code for visualizing the most similar images to the query, $\\boldsymbol{q}$, that will run once you implement some of the similarity measures. \n",
    "\n",
    "**Task 1.6:** Read your way through the code for plotting the results. You have to write a single line to get `most_similar_order` that should contain the indexes from $[1, 2, \\dots N]$ sorted according to the similarity measure in descending order.\n",
    "\n",
    "> *Hint:* Check out `np.argsort()` for getting the indices in sorted order. Is it sorted in ascending or descending order?\n",
    "\n",
    "> *Hint:* You can reverse the elements of an array by using `x[::-1]`. Hence if `x = [1, 2, 3]`, then `x[::-1]` is `[3, 2, 1]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1b163",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26c4443452e8593245582881671f5909",
     "grade": false,
     "grade_id": "cell-887428a60bcf7a9c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the number of top results to retrieve\n",
    "top_k = 5\n",
    "\n",
    "# Initialize figure\n",
    "fig = plt.figure(figsize=(3*top_k, 3*(len(similarity_functions)+1)))\n",
    "# Plot the query image\n",
    "ax = fig.add_subplot(len(similarity_functions)+1, 1, 1)\n",
    "ax.imshow(q_binarized.reshape(16, 16), cmap='gray')\n",
    "ax.set_title(r'$\\boldsymbol{q}$')\n",
    "ax.grid(False)\n",
    "plot_idx = top_k + 1 # update plot index\n",
    "\n",
    "# Iterate over the similarity functions and compute similarities\n",
    "for sim_func in similarity_functions:\n",
    "    # Compute similarities using the binarized images and query\n",
    "    similarities = sim_func(q_binarized, X_binarized)\n",
    "    \n",
    "    # Task 1.6: Use np.argsort to sort the similarities in descending order - save as \"most_similar_order\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Sort the images in the database\n",
    "    sorted_images = X[most_similar_order]\n",
    "\n",
    "    # Plot the top-k most similar images under the corresponding similarity function\n",
    "    for k in range(top_k):\n",
    "        # Create subplot for each image\n",
    "        ax = fig.add_subplot(len(similarity_functions)+1, top_k, plot_idx)\n",
    "        # Plot the image\n",
    "        ax.imshow(sorted_images[k].reshape(16, 16), cmap='gray')\n",
    "        # Set a title\n",
    "        ax.set_title(f'image #{most_similar_order[k]}\\nSimilarity: {similarities[most_similar_order[k]]:.2f}')\n",
    "        ax.grid(False)\n",
    "\n",
    "        # Add y-label if it's the first image in the row\n",
    "        if k == 0:\n",
    "            ax.set_ylabel(f'{sim_func.__name__.capitalize().replace(\"_\", \" \")}')\n",
    "\n",
    "        # Update plot index\n",
    "        plot_idx += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8312813e",
   "metadata": {},
   "source": [
    "Congratulations! <br> If your implementation is correct, we should be able to retrieve somewhat reasonable images using either of the measures and we have the basis of a recommender system.\n",
    "\n",
    "**Task 1.7:** Try to change the query image to e.g. $\\boldsymbol{q}=\\boldsymbol{x}_{13}$ (In Task 1.2). <br>Are the retrieved images still related to the query? Argue why / why not?\n",
    "\n",
    "> *Hint:* Make sure you remove the query image from X.\n",
    "\n",
    "- *Answer:* \n",
    "\n",
    "**Task 1.8:** How does the *least* similar images look? And what happens if you use $\\boldsymbol{X}$ and $\\boldsymbol{q}$ instead of $\\boldsymbol{X}_{\\text{binarized}}$ and $\\boldsymbol{q}_{\\text{binarized}}$ when computing the similarities?\n",
    "\n",
    "- *Answer:* \n",
    "\n",
    "> *Hint:* Don't reverse the order of the argsort you implemented previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d413e1d6",
   "metadata": {},
   "source": [
    "We will now investigate how scaling and translation impact the following three similarity measures: Cosine, ExtendedJaccard, and Correlation. \n",
    "\n",
    "**Task 1.9:** (Pen and Paper) Let $\\alpha$ and $\\beta$ be two constants. Determine which of the following statements are correct (you may need to use pen and paper). You can verify your statements with a coding example, we have provided a synthetic dataset in the code-cell below (x, y, a, b).\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\mathrm{Cosine}(\\boldsymbol{x},\\boldsymbol{y})&=\\mathrm{Cosine}(\\alpha\\boldsymbol{x},\\boldsymbol{y}) \\\\\n",
    "  \\mathrm{ExtendedJaccard}(\\boldsymbol{x},\\boldsymbol{y})&=\\mathrm{ExtendedJaccard}(\\alpha\\boldsymbol{x},\\boldsymbol{y}) \\\\\n",
    "  \\mathrm{Correlation}(\\boldsymbol{x},\\boldsymbol{y})&=\\mathrm{Correlation}(\\alpha\\boldsymbol{x},\\boldsymbol{y}) \\\\\n",
    "  \\mathrm{Cosine}(\\boldsymbol{x},\\boldsymbol{y})&=\\mathrm{Cosine}(\\beta+\\boldsymbol{x},\\boldsymbol{y}) \\\\\n",
    "  \\mathrm{ExtendedJaccard}(\\boldsymbol{x},\\boldsymbol{y})&=\\mathrm{ExtendedJaccard}(\\beta+\\boldsymbol{x},\\boldsymbol{y}) \\\\\n",
    "  \\mathrm{Correlation}(\\boldsymbol{x},\\boldsymbol{y})&=\\mathrm{Correlation}(\\beta+\\boldsymbol{x},\\boldsymbol{y}) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "> *Hint:* Even though a similarity measure is theoretically invariant e.g. to scaling, it might not be exactly invariant numerically.\n",
    "\n",
    "**Task 1.10:** Discuss the pratical implications of similarity measures that are translation and/or scaling invariant. You can base the discusison on the image digits dataset but think also of non-image example (e.g. retrieving documents based on the bag-of-words representation from last weeks exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd4b6f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3d9b71957bb299b89e6d57be969b240",
     "grade": false,
     "grade_id": "cell-ad23405ee8844150",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Generate two data objects with M random attributes\n",
    "M = 5\n",
    "x = np.random.rand(1, M)\n",
    "y = np.random.rand(1, M)\n",
    "\n",
    "# Two constants\n",
    "a = 1.5\n",
    "b = 1.5\n",
    "\n",
    "# Check the statements by computing LHS and RHS of the statements\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a26b9b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 2: Nearest neighbors method in Python\n",
    "\n",
    "In this exercise we will use the $k$-nearest neighbors (KNN) method for classification. First, we will explore how $k$-nearest neighbors work for 4 different synthetic datasets and how the choice of distance/dissimilarity measure used matters, depending on the dataset. We will consider *training* our $k$-nearest neighbors on a *training set* split of the dataset and later evaluate the learned model on a *test set* split of the dataset where we already split the datasets for you.\n",
    "\n",
    "**Task 2.1:** Examine the loaded `synth1_train.csv` and `synth1_test.csv` files using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43923eb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23caa57d32122787567e1c749d9b6eb9",
     "grade": false,
     "grade_id": "cell-19e450b9976c714d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b05aa0",
   "metadata": {},
   "source": [
    "From the datasets, construct $\\left(\\boldsymbol{X}_{\\text{train}}, \\boldsymbol{y}_{\\text{train}}\\right)$ and $\\left(\\boldsymbol{X}_{\\text{test}},\\boldsymbol{y}_{\\text{test}}\\right)$.\n",
    "\n",
    "> *Hint:* Remember how we split the dataframes into X and y in last weeks exercise? Which column-names corresponds to the attributes, and which column-name corresponds to the target label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a45f7d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d77f5a88ef573dad4aa8202d8bdceb0",
     "grade": false,
     "grade_id": "cell-afe7ad88f66a230b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Task 2.1: Load the dataset splits and construct the datamatrices X_train, X_test and target vectors y_train, y_test\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Check that the input shapes are correct\n",
    "assert X_train.shape[1] == 2, f\"Expected 2 features, but got {X_train.shape[1]}\"\n",
    "assert X_test.shape[1] == 2, f\"Expected 2 features, but got {X_test.shape[1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852616d0",
   "metadata": {},
   "source": [
    "Since the data is 2-dimensional, we can examine it closer by making a scatter plot of the points. We make a scatter plot of the points from the training data by directly leveraging Pandas' plotting tools and color the points based on their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training data, colored by the target variable\n",
    "df_train.plot(kind='scatter', x='x0', y='x1', c='y', cmap='viridis', title=f'Training set: {dataset_name}', figsize=(8, 6), edgecolor='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40699f5",
   "metadata": {},
   "source": [
    "For creating a classification model using $k$-nearest neighbors we need to define what it means to be close neighbors - or in other words, **how we define similarity between points**. More specifically, we need to choose:\n",
    "- the number of neighbors, $k$\n",
    "- a distance measure, $d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)$\n",
    "\n",
    "We remark that distance measures are inversely related to similarity measures by definition. As an example, we can convert cosine similarity to a distance measure by $1 - \\mathrm{Cosine}(\\boldsymbol{x}_i, \\boldsymbol{x}_j)$ since the maximum cosine similarity is 1. <br>I.e. the smaller the cosine similarity, the larger the distance.\n",
    "\n",
    "If we want to classify some new observation, $\\boldsymbol{x}^\\ast$, using $k$-nearest neighbors, we then have to:\n",
    "1) compute the distance from $\\boldsymbol{x}^\\ast$ to all points, $\\boldsymbol{x}_i$, from the training data, i.e. $d(\\boldsymbol{x}^\\ast, \\boldsymbol{x}_i)$ where $\\boldsymbol{x}_i$ is the $i$'th row in $\\boldsymbol{X}_{\\text{train}}$.\n",
    "2) find the $k$ nearest data points in $\\boldsymbol{X}_{\\text{train}}$ and get their labels.\n",
    "3) classify $\\boldsymbol{x}^\\ast$ according to the majority label of the $k$ nearest neighbors.\n",
    "\n",
    "As you have seen in the lecture, a common distance measure is the **general Minkowski distance** also known as the **$p$-norm**:\n",
    "$$\n",
    "    d_p\\left(\\boldsymbol{x}^\\ast, \\boldsymbol{x}\\right) = \\left( \\sum_{j=1}^M \\lvert x^\\ast_j - x_{j}\\rvert^p\\right)^{\\frac{1}{p}}\n",
    "$$\n",
    "which reduces to the cityblock (or Manhattan) distance for $p=1$ and the Euclidean distances for $p=2$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{Cityblock}: \\ d_1\\left(\\boldsymbol{x}^\\ast, \\boldsymbol{x}\\right) = \\sum_{j=1}^M \\lvert x^\\ast_j - x_{j}\\rvert \\qquad \\qquad\n",
    "    \\text{Euclidean}: \\ d_2\\left(\\boldsymbol{x}^\\ast, \\boldsymbol{x}\\right) = \\sqrt{\\sum_{j=1}^M \\lvert x^\\ast_j - x_{j}\\rvert^2}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b256cd60",
   "metadata": {},
   "source": [
    "We will now show you how to use the `KNeighborsClassifier` from `sklearn`, to train a $k$-Nearest Neighbors classifier on our dataset. <br>We will use $k=5$ neighbours and base the distance on euclidean distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7707b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of neighbours\n",
    "k = 5\n",
    "\n",
    "# Create the model object with the specific hyperparameters\n",
    "model = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "\n",
    "# \"Train\" the model by giving it the training data\n",
    "model.fit(X_train, y_train) # i.e. the attributes and the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f73c97",
   "metadata": {},
   "source": [
    "**Task 2.2:** Consider a new observation $\\boldsymbol{x}^\\ast = [1, -1]^\\top$. Which label would you assign to this point based on the training data?\n",
    "\n",
    "> *Hint:* Take a look at the scatter plot of the training data above. \n",
    "\n",
    "- *Answer:*\n",
    "\n",
    "\n",
    "**Task 2.3:** Use `KNeighborsClassifier` from `sklearn` to classify the new observation, save the predicted label in a variable called `y_pred`. <br> Does the predicted label match your answer from Task 2.2?\n",
    "\n",
    "> *Hint:* Use model.predict(). Checkout the documentation for `sklearn.neighbors.KNeighborsClassifier` if you need more information.\n",
    "\n",
    "- *Answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20613a3e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d0c805057e78030a150d305084a33f3",
     "grade": false,
     "grade_id": "cell-279e83d6c4df3c23",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "X_new = np.array([[1, -1]])\n",
    "\n",
    "# Task 2.3: Make predictions on the new observation X_new, save the result in y_pred.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(f\"y_pred is predicted to have label {y_pred.item()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ef82d",
   "metadata": {},
   "source": [
    "We have provided a plot of the training data, and the new observation $\\boldsymbol{x}_{\\text{new}}$ in the plot below. <br> The plot should also show the $k$ closest neighbors to $\\boldsymbol{x}_{\\text{new}}$ - you need to implement this.\n",
    "\n",
    "**Task 2.4:** Find the closest neighbors to $\\boldsymbol{x}_{\\text{new}}$ and their labels, save them as `closest_neighbors` and `closest_labels`.\n",
    "\n",
    "> *Hint:* You can find the indices of the closest neighbors using the `closest_neighbors_indices = model.kneighbors(X_new, n_neighbors=k, return_distance=False).flatten()`\n",
    "\n",
    "> *Hint:* Use the indices to find the closest neighbors and their labels, by indexing the training data and labels, X_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f93224",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fde9c1d1aac0a5054f288162c5e2c4e6",
     "grade": false,
     "grade_id": "cell-f8020a824bc83e06",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 2.4: Find the closest neighbors to the new point and their labels, save them as \"closest_neighbors\" and \"closest_labels\".\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Visualize the training data, colored by the target variable\n",
    "df_train.plot(kind='scatter', x='x0', y='x1', c='y', cmap='viridis', title=f'Training set: {dataset_name}', figsize=(8, 6), edgecolor='gray')\n",
    "\n",
    "# Plot the new point and its closest neighbors\n",
    "plt.scatter(X_new[0, 0], X_new[0, 1], color='red', marker='o', s=50, label='New Point')\n",
    "plt.scatter(closest_neighbors[:, 0], closest_neighbors[:, 1], color='k', marker='x', s=50, label='Closest Neighbors')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Closest labels: {closest_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645294b8",
   "metadata": {},
   "source": [
    "We are interested in determining how well this classification method works. To do so, we use it to classify the test dataset $\\boldsymbol{X}_{\\text{test}}$ and compare the predictions with the true test labels $\\boldsymbol{y}_{\\text{test}}$.\n",
    "\n",
    "**Task 2.5:** Compute the predicted labels for all data points in $\\boldsymbol{X}_{\\text{test}}$, save them as `y_pred`.\n",
    "\n",
    "> *Hint:* The `model.predict`-method works both on individual data points and on entire datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3239bd8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e43016c05817a1ae401aab6a389587e7",
     "grade": false,
     "grade_id": "cell-6278927a1646a194",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 2.5: Predict using the model, save the predictions in y_pred\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(f\"Predictions: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3280616a",
   "metadata": {},
   "source": [
    "We now compute the accuracy and error rate. \n",
    "\n",
    "**Task 2.6:** Compute the accuracy and error rate of the model, save them as `accuracy` and `error_rate`.\n",
    "\n",
    "> *Hint:* You can compare `y_test` and `y_pred` with the `==` operator, this returns a boolean array of whether each element is equal. You can then use the `np.mean()` function to compute how often the model predicted the correct class.\n",
    "\n",
    "> *Hint:* Computing the error rate is quite simple - How does the error rate relate to the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2494a400",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb6a4c5772212793ba0ceaf5912eac12",
     "grade": false,
     "grade_id": "cell-bd4161feedd39e7f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 2.6: Compute the accuracy and error rate, save them as accuracy and error_rate.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"Accuracy: {0}%\".format(accuracy * 100))\n",
    "print(\"Error rate: {0}%\".format(error_rate * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb270f8",
   "metadata": {},
   "source": [
    "\n",
    "**Task 2.7:** Generate the confusion matrix, save it as `cm`. We will handle the plotting. <br>How well does the model perform? Does it perform specifically better/worse on some classes than others?\n",
    "\n",
    "> *Hint:* To generate a confusion matrix, you can use the function `confusion_matrix()` function from the `sklearn.metrics` module. <br> You can find the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c4cd8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4d75e4fe0a0f7f35d4236940b05f349",
     "grade": false,
     "grade_id": "cell-fa66fbf8f3ced1a0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Task 2.7: Compute the elements of the confusion matrix\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, cmap=\"binary\", interpolation=\"None\")\n",
    "plt.colorbar()\n",
    "plt.xticks(np.unique(y_test))\n",
    "plt.yticks(np.unique(y_test))\n",
    "plt.xlabel(\"Predicted class\")\n",
    "plt.ylabel(\"Actual class\")\n",
    "plt.title(\"Confusion matrix (Accuracy: {0}%, Error Rate: {1}%)\".format(accuracy, error_rate))\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center', color='white')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640e8896",
   "metadata": {},
   "source": [
    "In addition to this, we might be able to say something qualitatively about the model's performance by visualizing the **decision boundary** - that is how the 2D input space is separated by the $k$-nearest neighbors model. <br> We can do so, by predicting the label for every input point $\\boldsymbol{x}^\\ast$ in a fine grid over the input space. \n",
    "\n",
    "In the code below, we define the grid of $500\\times500$ points over the input space and stack them into the data matrix format of shape $(500 \\times 500)\\times 2$. We can then predict on our data-matrix, `X_grid`, and store the predicted labels in `y_pred` - that'll be your job. \n",
    "\n",
    "**Task 2.5:** Visualize the decision boundary by predicting the label for every input point $\\boldsymbol{x}^\\ast$ in `X_grid` and storing the predicted labels in `y_pred`. Then reshape the predicted labels into shape `resolution` $\\times$ `resolution`, such that they can be visualized using `plt.imshow()`. \n",
    "\n",
    "> *Hint:* Apply your $k$-nearest neighbors model to every point on the grid and store the labels. Then reshape the predicted labels into shape `resolution` $\\times$ `resolution` and plot the decision boundary using `plt.imshow()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f0df7e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df9ce6cd0cdb22112da7ce97f02fc699",
     "grade": false,
     "grade_id": "cell-1d6b30b3dad3070f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define resolution of the grid, i.e. how many points per axis\n",
    "resolution = 500\n",
    "# Construct the grid\n",
    "min_x0, max_x0 = X_train[:, 0].min() - 0.5, X_train[:, 0].max() + 0.5\n",
    "min_x1, max_x1 = X_train[:, 1].min() - 0.5, X_train[:, 1].max() + 0.5\n",
    "X_grid = np.meshgrid(\n",
    "    np.linspace(min_x0, max_x0, resolution),\n",
    "    np.linspace(min_x1, max_x1, resolution)\n",
    ")\n",
    "# Stack the grid points into the format of X, i.e. shape N x M\n",
    "X_grid = np.stack([X_grid[0].ravel(), X_grid[1].ravel()]).T\n",
    "\n",
    "# Task 2.5: Predict the labels for the grid points, save as y_pred\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n",
    "# Plot the decision boundary\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Decision boundary of kNN')\n",
    "ax.imshow(y_pred, extent=(min_x0, max_x0, min_x1, max_x1), origin='lower', alpha=0.5, cmap='viridis')\n",
    "# Plot the training points\n",
    "df_train.plot(kind='scatter', x='x0', y='x1', c='y', cmap='viridis', ax=ax, edgecolors='gray', colorbar=False, label='Training data')\n",
    "df_test.plot(kind='scatter', x='x0', y='x1', c='y', cmap='viridis', ax=ax, edgecolors='gray', colorbar=False, marker='s', label='Test data')\n",
    "ax.set_aspect('auto')\n",
    "ax.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0f91df",
   "metadata": {},
   "source": [
    "**Task 2.6:** Go back and experiment with the distance measure and number of neighbors and run the experiment for the other synthetic datasets too. As distance measures, consider cityblock, Euclidean and cosine as discussed in Part 1 (specifically useful for the `synth2` dataset). Which distance measures worked best for the four problems? Can you explain why? How many neighbors were needed for the four problems? Can you give an example of when it would be good to use a large/small number of neighbors? Consider e.g. when clusters are well separated versus when they are overlapping.\n",
    "\n",
    "> *Hint:* For `synth3` you might find the Mahalanobis distance useful. Use `model = KNeighborsClassifier(n_neighbors=k, metric='mahalanobis', metric_params={'V': np.cov(X_train, rowvar=False)})` for doing so. You can find further details on this distance metric [on this link](https://en.wikipedia.org/wiki/Mahalanobis_distance).\n",
    "\n",
    "- *Answer:* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6caea58",
   "metadata": {},
   "source": [
    "### Regression with $k$-nearest neighbors\n",
    "\n",
    "In the previous example we used $k$-nearest neighbors for solving a classification problem, but the method can just as well be used for modeling a regression task. We follow a similar algorithmic approach as before when predicting the regression output of a new observation, $\\boldsymbol{x}^\\ast$, that is:\n",
    "1) compute the distance from $\\boldsymbol{x}^\\ast$ to all points, $\\boldsymbol{x}_i$, from the training data, i.e. $d(\\boldsymbol{x}^\\ast, \\boldsymbol{x}_i)$ where $\\boldsymbol{x}_i$ is the $i$'th row in $\\boldsymbol{X}_{\\text{train}}$.\n",
    "2) find the $k$ nearest data points in $\\boldsymbol{X}_{\\text{train}}$ and get the associated target attribute values from $\\boldsymbol{y}_{\\text{train}}$.\n",
    "3) average the associated target attribute values of the $k$ nearest neighbors as the predicted output.\n",
    "\n",
    "We provide a simple example in 1D below for a sinusoidal function with noisy observations in the range $x\\in[0,1]$. First, we compute the prediction of $x^\\ast = 0.5$ and show its $k=5$ nearest neighbors. Next, we compute predictions for a fine grid of points in the range of $x$ and plot the results as our **approximated function**. \n",
    "\n",
    "**Task 2.7:** Read your way through the code and understand the details of this toy experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef788329",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "# Experiment parameters\n",
    "k = 5                       # number of neighbors\n",
    "N = 20                      # number of points\n",
    "observation_noise = 0.25    # standard deviation of observation noise\n",
    "\n",
    "# Define the true (unseen) function\n",
    "def sine_function(x, frequency: int = 5):\n",
    "    return np.sin(frequency * x)\n",
    "\n",
    "# Define a new test point\n",
    "X_new = np.array([[0.5]])\n",
    "\n",
    "# Generate training data\n",
    "X_train = np.random.rand(N, 1) # Sample training data points\n",
    "y_train = sine_function(X_train) + np.random.normal(0, observation_noise, (N, 1)) # Get target values for training points, with observation noise\n",
    "\n",
    "# Compute the steps of k-nearest neighbors\n",
    "# Note that all general Minkowski distances can be computed using the same method in 1D - can you argue why?\n",
    "distances = np.linalg.norm(X_train - X_new, axis=1)\n",
    "most_similar_idxs = np.argsort(distances)[:k]  # Get the indices of the k nearest neighbors\n",
    "y_pred = y_train[most_similar_idxs].mean()  # Predict the output for the new input\n",
    "\n",
    "# Make a subplot layout\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5), sharex=True, sharey=True)\n",
    "\n",
    "# Plot training data\n",
    "axs[0].plot(X_train, y_train, 'o', label='Training data') \n",
    "# Plot the results of kNN for a single point\n",
    "axs[0].vlines(X_new, ymin=y_train.min(), ymax=y_train.max(), color='r', linestyle='--', label=fr'New point, $x^\\ast={X_new.item()}$')\n",
    "# Highlight the nearest neighbors\n",
    "axs[0].plot(X_train[most_similar_idxs], y_train[most_similar_idxs], 'o', color='orange', label=f'$k$={k} nearest neighbors') \n",
    "# Plot the predicted value\n",
    "axs[0].plot(X_new, y_pred, 'ro', mec='k', label=fr'Predicted value, $\\hat{{y}}={y_pred.item():.2f}$') \n",
    "# Plot the true function\n",
    "axs[0].plot(np.linspace(0, 1, 100), sine_function(np.linspace(0, 1, 100)), label='True function', color='k') \n",
    "# Add figure layout elements\n",
    "axs[0].set_title('kNN in 1D (single point example)')\n",
    "axs[0].set_xlabel('x')\n",
    "axs[0].set_ylabel('y')\n",
    "axs[0].legend()\n",
    "\n",
    "\n",
    "# Now we run the same computation but for a fine grid of test points in the interval [0, 1]\n",
    "X_pred = np.linspace(0, 1, 1000).reshape(-1, 1)  # Create a grid of test points\n",
    "distances = np.linalg.norm(X_train[None, :] - X_pred[:, None], axis=2) # vectorized version of computing distances for all test points at once\n",
    "most_similar_idxs = np.argsort(distances, axis=1)[:, :k]  # Get the indices of the k nearest neighbors for each test point\n",
    "y_pred = y_train[most_similar_idxs].mean(axis=1)  # Predict the output for each test point\n",
    "\n",
    "# Plot the results of kNN on the range [0,1]\n",
    "# Plot training data\n",
    "axs[1].plot(X_train, y_train, 'o', label='Training data') \n",
    "# Plot the true function\n",
    "axs[1].plot(np.linspace(0, 1, 100), sine_function(np.linspace(0, 1, 100)), label='True function', color='k') \n",
    "# Plot the predicted function in the range x in [0,1]\n",
    "axs[1].plot(X_pred, y_pred, label='Aprroximated function', color='red') \n",
    "axs[1].set_title(r'Approximated function by kNN, $x^\\ast \\in [0,1]$')\n",
    "axs[1].set_xlabel('x')\n",
    "axs[1].set_ylabel('y')\n",
    "axs[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0978ac5b",
   "metadata": {},
   "source": [
    "**Task 2.7:** Can you explain why the approximated function is a step-wise function? Experiment with the number of neighbors $k$ - what happens if $k=1$ or $k=10$? What happens to the approximate function if you increase the number of samples to $N=200$ in combination with $k=20$? Can you explain what happens to the quality of the fit and why?\n",
    "\n",
    "- *Answer:*\n",
    "\n",
    "In general we can use a concept called **cross-validation** to select the optimal distance metric and number of nearest neighbors $k$ although this can be computationally expensive. We will cover this topic in detail in week 6. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c1c8b2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 3: KNN Classification on Wine data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11093b86",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"display: flex; align-items: flex-start;\">\n",
    "\n",
    "<div style=\"flex: 1; padding-right: 5em;\">\n",
    "\n",
    "We will in this part of the exercise consider two datasets related to red and white variants of the Portuguese \"Vinho Verde\" wine, the data has been downloaded from [this website](http://archive.ics.uci.edu/ml/datasets/Wine+Quality). Only physicochemical and sensory attributes are available, i.e., there is no data about grape types, wine brand, wine selling price, etc. You can see the attributes of the data in the provided table. Attributes 111 are based on physicochemical tests and attribute 12 on human judging. \n",
    "\n",
    "The aim of this exercise is to implement a machine learning pipeline for classifying the color of the wine. We will use visualization and distance measurements to identify outliers and remove these outliers from the data. It might be necessary to remove some outliers before other outlying observations become visible. Thus, the process of finding and removing outliers is often iterative.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"flex: 0 0 350px;\">\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>#</th>\n",
    "<th>Attribute</th>\n",
    "<th>Unit</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>1</td><td>Fixed acidity (tartaric)</td><td>g/dm</td></tr>\n",
    "<tr><td>2</td><td>Volatile acidity (acetic)</td><td>g/dm</td></tr>\n",
    "<tr><td>3</td><td>Citric acid</td><td>g/dm</td></tr>\n",
    "<tr><td>4</td><td>Residual sugar</td><td>g/dm</td></tr>\n",
    "<tr><td>5</td><td>Chlorides</td><td>g/dm</td></tr>\n",
    "<tr><td>6</td><td>Free sulfur dioxide</td><td>mg/dm</td></tr>\n",
    "<tr><td>7</td><td>Total sulfur dioxide</td><td>mg/dm</td></tr>\n",
    "<tr><td>8</td><td>Density</td><td>g/cm</td></tr>\n",
    "<tr><td>9</td><td>pH</td><td>pH</td></tr>\n",
    "<tr><td>10</td><td>Sulphates</td><td>g/dm</td></tr>\n",
    "<tr><td>11</td><td>Alcohol</td><td>% vol.</td></tr>\n",
    "<tr><td>12</td><td>Quality score</td><td>010</td></tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c557f326",
   "metadata": {},
   "source": [
    "**Task 3.1:** Load the wine dataset from the associated data folder. Construct the data matrix $\\boldsymbol{X}$ and the target attribute $\\boldsymbol{y}$ where the target attribute is the wine color, `Color`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc07451",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10ff7eeb35ed913fee8d9513784a2b23",
     "grade": false,
     "grade_id": "cell-0f2a98fd72167e22",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 3.1: Load the wine dataset, split into attributes, X, and labels, y.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "assert X.shape == (6497, 12), \"There should be 6497 samples and 12 features in the wine dataset\"\n",
    "assert y.shape == (6497,), \"There should be 6497 labels in the wine dataset\"\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04b57c",
   "metadata": {},
   "source": [
    "**Task 3.2:** Plot a boxplot of each attribute in $\\boldsymbol{X}$ and compute summary statistics of the data matrix $\\boldsymbol{X}$. What does the mean vs. median for the \"Alcohol\" attribute tell you?\n",
    "\n",
    "> *Hint:* Remember that since `X` is a Pandas dataframe, you can do `X.plot(kind='box')`. <br> You might want to check the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) of the `plot` method of a Pandas dataframe - specifically the arguments `subplots` and `layout` to make the boxplots look better.\n",
    "\n",
    "> *Hint:* Consider using the Pandas method `.describe()` - it computes several summary statistics at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fdef6d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7440b1e0fc78a0788b2c9b2be6cedd06",
     "grade": false,
     "grade_id": "cell-10c040273a1a9a01",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 3.2: Plot a boxplot of the attributes in X, and compute summary statistics\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0a1213",
   "metadata": {},
   "source": [
    "As you can hopefully see from the boxplot and summary statistics, the dataset has many observations that can be considered outliers and in order to carry out analyses later it is important to remove the corrupted observations, e.g. $10^{15}$ is clearly not a proper value for alcohol content! However, it is impossible to see the distribution of the data, because the axis is dominated by these extreme outliers. Say we expect volatide acidity (VA) to be around $0-2 \\textrm{g}/\\textrm{dm}^3$, density (D) to be close to $1 \\textrm{g}/\\textrm{cm}^3$, and alcohol percentage (AP) to be somewhere between $5-20\\% \\textrm{vol}$. We can use this knowledge to define wines with VA $> 2$, D $> 1$ and AP $> 20$ as outliers.\n",
    "\n",
    "Below, we have defined a mask that marks the outliers, based on our expert assumption. \n",
    "\n",
    "**Task 3.3:** Remove outliers from $\\boldsymbol{X}$ and $\\boldsymbol{y}$ by applying the inverted mask. Show the boxplot for each attribute after removing these outliers. \n",
    "\n",
    "> *Hint:* Pandas allows for logical conditions like `(df['Alcohol'] > 20)` to identify rows with outliers. Combine multiple conditions using the `|` operator (logical OR), and wrap each condition in parentheses. To remove the outliers, consider using the `~` operator to invert the filter.\n",
    "\n",
    "> *Hint:* You can use a mask to index outlier observations by e.g. writing `X=X[mask,:]` and `y=y[mask]` where `mask` indicates the data objects that should be maintained. In our case, the mask is outliers, and should therefore be inverted. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a18b68",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ca4fb2f5aa48139a72896725a58eaf5",
     "grade": false,
     "grade_id": "cell-1e199a691f2f28aa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the boolean mask for outlier detection\n",
    "mask = (X['Volatile acidity'] > 2) | (X['Density'] > 1) | (X['Alcohol'] > 20)\n",
    "\n",
    "# Task 3.3: Use the mask to remove the outliers from X and y. Plot a boxplot like you did in the previous task. \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "assert X.shape == (6304, 12), \"There should be 6304 samples and 12 features in the wine dataset after outlier removal\"\n",
    "assert y.shape == (6304,), \"There should be 6304 labels in the wine dataset after outlier removal\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e89b2d",
   "metadata": {},
   "source": [
    "We will now attempt to classify the type of wine i.e. white or red (and later also the quality score) based on the physicochemical tests. Visual inspection of the data can give an indication of the difficulty of these tasks. In the following code cell, we plot a matrix scatter plot for all attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of attributes\n",
    "M = X.shape[1]\n",
    "\n",
    "# Plot a matrix scatter plot of the wine attributes, colored by the wine color\n",
    "fig, axs = plt.subplots(M, M, figsize=(20, 20), sharex='col', sharey='row')\n",
    "for i in range(M):\n",
    "    for j in range(M):\n",
    "        for color in y.unique(): # loop through each label\n",
    "            # Construct a mask based on the label\n",
    "            mask = (y == color)\n",
    "            # Plot the scatter plot for attribute pair (if not on the diagonal)\n",
    "            axs[i, j].scatter(\n",
    "                x=X[mask].iloc[:, j],        # x-values for the $j$'th attribute\n",
    "                y=X[mask].iloc[:, i],        # y-values for the $i$'th attribute\n",
    "                label=color, alpha=0.3,\n",
    "                color='r' if color == 'Red' else 'y'\n",
    "            )\n",
    "\n",
    "# Update titles\n",
    "for col in range(M):\n",
    "    axs[0, col].set_title(X.columns[col])\n",
    "    axs[col, 0].set_ylabel(X.columns[col])\n",
    "\n",
    "# Add the legend to the last subplot only\n",
    "axs[0,0].legend(loc='upper left')\n",
    "plt.tight_layout(pad=1.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658d2b1",
   "metadata": {},
   "source": [
    "**Task 3.4:** Do any of the attributes appear to correlate with each other? \n",
    "\n",
    "- *Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed3bab",
   "metadata": {},
   "source": [
    "**Task 3.5:** Validate your findings by computing the correlation matrix between all attributes and the target attribute. \n",
    "\n",
    "> *Hint:* In order to compute the correlation between the target variable `y` and the rest of the attributes, `X`, we need to convert the target attribute `y` to numerical values - we have done this for you. \n",
    "\n",
    "> *Hint:* The easiest way is to construct a modified Pandas dataframe, i.e. `df_tilde`, by concatenating `X_tilde` and `y_numerical` using `pd.concat()`.\n",
    "\n",
    "> *Hint:* Use the Pandas method `.corr()` on your new dataframe `df_tilde`, to compute the correlation matrix - save it as `correlation_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc762de",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5f3fac802488c710535083e5747b1a5",
     "grade": false,
     "grade_id": "cell-7cf91f02c25c333e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Transform the target variable into a numerical format\n",
    "y_numerical = y.astype(\"category\").cat.codes\n",
    "\n",
    "# Task 3.5: Compute the correlation matrix of the data, save it as \"correlation_matrix\".\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "fig.suptitle('Correlation matrix of the standardized data', fontsize=16)\n",
    "# Plot correlation matrix and set colorbar min and max to -1 and 1 since thats the range of the correlation coefficients\n",
    "plt.imshow(correlation_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.xticks(ticks=np.arange(correlation_matrix.shape[1]), labels=list(X.columns) + [\"Wine color\"], rotation=90)\n",
    "plt.yticks(ticks=np.arange(correlation_matrix.shape[1]), labels=list(X.columns) + [\"Wine color\"])\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe33e6",
   "metadata": {},
   "source": [
    "**Task 3.6:** There indeed seems to be correlation between the target attribute and some input attributes! which of the physicochemical measurements seem to be well suited in order to discriminate between red and white wines? \n",
    "- *Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b84ef88",
   "metadata": {},
   "source": [
    "We will go ahead try fitting a $k$-nearest neighbor to the data in $\\tilde{\\boldsymbol{X}}$. We proceed as follows:\n",
    "\n",
    "1) We remove the \"Quality score\" attribute - as this is not a physiochemical attribute. \n",
    "2) We construct a random training and test set split of the data in order to evaluate on *unseen* data \n",
    "3) We standardize the training and test data according to the statistics of the **training data**. We want no information leaking from the training data to the test data as this will bias the model performance - but more on that in a later week!\n",
    "\n",
    "The code for these three steps is provided in the cell below.\n",
    "\n",
    "**Task 3.7:** Initiate, fit and predict using a $k$-nearest neighbor with $k=5$ and Euclidean distance measure to the training data using `sklearn` - just like you've seen above.\n",
    "\n",
    "> *Hint:* Remember to fit the model on the training data (`X_train`, `y_train`) and predict on the test data (`X_test`).\n",
    "\n",
    "> *Hint:* Save the predicted labels in a variable called `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f8229b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4871473e8a72ed3c56540886454fc91",
     "grade": false,
     "grade_id": "cell-ae6688a88f02413a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets of proportion 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.drop(columns=[\"Quality score (0-10)\"]), y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Compute mean and standard deviation of each attribute of the training dataset\n",
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)\n",
    "\n",
    "# Standardize the training and test data. This ensures that the models are not biased towards \n",
    "# any particular feature due to differences in scale, which could impact model performance, especially for distance-based algorithms.\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "# Task 3.7: Initialize, fit and predict using the KNN classifier - save the predicted labels as y_pred\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff9df11",
   "metadata": {},
   "source": [
    "**Task 3.8:** Compute the accuracies, error rates and confusion matrices of the model. What happens if you change the hyperparameters kNN?\n",
    "\n",
    "> *Hint:* You have seen how to do this before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b282b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c272c18d650e45bfdfe8a2c07ca4a9b6",
     "grade": false,
     "grade_id": "cell-28b41c62dab366eb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 3.8: Compute the accuracies, error rates and confusion matrices of the model - save the results in the variables accuracy, error_rate and cm\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", ax=ax, cmap=\"Blues\", cbar=False)\n",
    "ax.set_title(f\"kNN - Confusion Matrix \\n (Accuracy: {accuracy * 100:.2f}%, Error Rate: {error_rate * 100:.2f}%)\")\n",
    "\n",
    "ax.set_xticks([0.5, 1.5], np.unique(y_test))\n",
    "ax.set_yticks([0.5, 1.5], np.unique(y_test))\n",
    "ax.set_xlabel(\"Predicted class\")\n",
    "ax.set_ylabel(\"Actual class\")\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9747bbd0",
   "metadata": {},
   "source": [
    "**Task 3.9:** Try to comment out the standardization step. What happens to the model performance? Can you explain why?\n",
    "\n",
    "- *Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb5f44",
   "metadata": {},
   "source": [
    "Let's say we obtain a new wine with the following attribute values, we have saved it as a variable called `X_new`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ade2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new data object (new type of wine) with the attributes given in the text\n",
    "X_new = pd.DataFrame([[6.9, 1.09, 0.06, 2.1, 0.0061, 12, 31, 0.99, 3.5, 0.44, 12]], columns=X_train.columns)\n",
    "X_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3560adb",
   "metadata": {},
   "source": [
    "**Task 3.10:** Show that the new wine would be classified as Red by the $k$-nearest neighbor model - save the result as `y_pred`. \n",
    "\n",
    "> *Hint:* Remember that the data has been standardized, so the new wine has to be standardized as well according to the training data.\n",
    "\n",
    "> *Hint:* Use the `predict` method of your previously trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7bd2d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f8c87b03bc2f6a603fd7639afc8e1e3",
     "grade": false,
     "grade_id": "cell-bdfd3f86a69be48d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Task 3.10: Predict the class of the new wine\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Print predictions and probabilities\n",
    "print(f\"Predicted class for the new wine using kNN: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785f6590",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Assignment 2:\n",
    "\n",
    "In this weeks Assignment we will be taking a look at the Whiskies dataset. The dataset in `data/whiskies.csv` is data from a number of whisky distilleries. For each distillery, there is a set of subjective judgements about the flavour characteristics of their product. The data comes from [this Strathclyde University research project](https://www.mathstat.strath.ac.uk/outreach/nessie/nessie_whisky.html).\n",
    "\n",
    "Each distillery has been judged on twelve flavour indicators (like \"smokiness\" or \"sweetness\"), and they have been assigned values from 0-4, indicating the strength of that category as judged by an expert whisky drinker. These can be seen as 12D vectors, one vector per distillery. **Every distillery is represented as a point in twelve dimensional vector space.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "231f5923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform info: <<<['machine-guid:c919e4f9-6ff0-4e4d-bb54-ba7bdb0774e2', 'mac:50eb71323336', 'node:MSOStudie', 'system:Windows', 'release:10', 'machine:AMD64', 'processor:Intel64 Family 6 Model 142 Stepping 12, GenuineIntel']:d0e4e4e985eb4488dbd98f99c5dc4ea1f65e7719e12ce22450fa25feb5ab25ab:20260210122137:aed311dad1ebecf89bd3c71d0d15cffd78a57a0a97504adf8a88462d22ce7e5a:python:3.11.9|platform:Windows-10-10.0.19045-SP0|numpy:2.4.2|sklearn:1.8.0|pandas:3.0.0|torch:2.10.0+cpu>>>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"\n",
       "            padding:10px 12px;\n",
       "            margin:8px 0;\n",
       "            border:1px solid #c3e6cb;\n",
       "            background:#d4edda;\n",
       "            color:#155724;\n",
       "            border-radius:6px;\n",
       "            font-family:sans-serif;\n",
       "        \">\n",
       "            <div style=\"margin:1px; font-weight:600;\"> Test passed</div>\n",
       "            \n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We import everything needed for the assignment part\n",
    "import matplotlib.pyplot as plt\n",
    "# Plotting style\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_theme(font_scale=1.)\n",
    "\n",
    "import utils as utils, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "utils.reset_marks()\n",
    "a,b,c,d = utils.platform_info()\n",
    "print(f\"Platform info: <<<{a}:{b}:{c}:{d}>>>\")\n",
    "with utils.marks(0): # If we get to this point, we will show a checkmark to indicate that everything looks fine so far\n",
    "    assert(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f6476",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Assignment 2.0:** Fill in your student ID and your full name in the variables below (and ensure that the cell runs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d27ada5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40f16e0fc886fa10801d788a65ea3d6e",
     "grade": false,
     "grade_id": "cell-ba4e21d640a41143",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "student_id = \"s194023\" \n",
    "student_typewritten_signature = \"Magnus Ottosen\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c09037",
   "metadata": {},
   "source": [
    "Our first small test is to check that the format of the student id is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b978e090",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de5aa1371a4980e3fecaba342aafcc63",
     "grade": true,
     "grade_id": "cell-15b3378255cad396",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"\n",
       "            padding:10px 12px;\n",
       "            margin:8px 0;\n",
       "            border:1px solid #c3e6cb;\n",
       "            background:#d4edda;\n",
       "            color:#155724;\n",
       "            border-radius:6px;\n",
       "            font-family:sans-serif;\n",
       "        \">\n",
       "            <div style=\"margin:1px; font-weight:600;\"> Test passed</div>\n",
       "            \n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We include such simple tests (without marks) to make sure you are on the right track\n",
    "with utils.marks(0): \n",
    "    assert(re.compile(r\"^s\\d{6}$\").match(student_id))\n",
    "    assert(len(student_typewritten_signature)>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d665a",
   "metadata": {},
   "source": [
    "We will now load the dataset. As part of the dataset is both the twelve flavour indicators, as well as the latitude and longitude of the distilleries. <br>We remove the distillery names, and the latitude and longitude from the DataFrame, and save them as `distilleries` and `locations` variables - while we save the observations as `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38f1995d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowID</th>\n",
       "      <th>Distillery</th>\n",
       "      <th>Body</th>\n",
       "      <th>Sweetness</th>\n",
       "      <th>Smoky</th>\n",
       "      <th>Medicinal</th>\n",
       "      <th>Tobacco</th>\n",
       "      <th>Honey</th>\n",
       "      <th>Spicy</th>\n",
       "      <th>Winey</th>\n",
       "      <th>Nutty</th>\n",
       "      <th>Malty</th>\n",
       "      <th>Fruity</th>\n",
       "      <th>Floral</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Aberfeldy</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>\\tPH15 2EB</td>\n",
       "      <td>286580</td>\n",
       "      <td>749680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Aberlour</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>\\tAB38 9PJ</td>\n",
       "      <td>326340</td>\n",
       "      <td>842570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>AnCnoc</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>\\tAB5 5LI</td>\n",
       "      <td>352960</td>\n",
       "      <td>839320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Ardbeg</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>\\tPA42 7EB</td>\n",
       "      <td>141560</td>\n",
       "      <td>646220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Ardmore</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>\\tAB54 4NH</td>\n",
       "      <td>355350</td>\n",
       "      <td>829140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowID Distillery  Body  Sweetness  Smoky  Medicinal  Tobacco  Honey  Spicy  \\\n",
       "0      1  Aberfeldy     2          2      2          0        0      2      1   \n",
       "1      2   Aberlour     3          3      1          0        0      4      3   \n",
       "2      3     AnCnoc     1          3      2          0        0      2      0   \n",
       "3      4     Ardbeg     4          1      4          4        0      0      2   \n",
       "4      5    Ardmore     2          2      2          0        0      1      1   \n",
       "\n",
       "   Winey  Nutty  Malty  Fruity  Floral     Postcode   Latitude   Longitude  \n",
       "0      2      2      2       2       2   \\tPH15 2EB     286580      749680  \n",
       "1      2      2      3       3       2   \\tAB38 9PJ     326340      842570  \n",
       "2      0      2      2       3       2    \\tAB5 5LI     352960      839320  \n",
       "3      0      1      2       1       0   \\tPA42 7EB     141560      646220  \n",
       "4      1      2      3       1       1   \\tAB54 4NH     355350      829140  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/whiskies.txt\")\n",
    "\n",
    "# Displaying the dataframe.\n",
    "display(df.head())\n",
    "\n",
    "# Removing the names and the location, and saving them in their own variables\n",
    "distilleries = df[\"Distillery\"]\n",
    "locations = np.array(df.iloc[:, -2:])\n",
    "X = df.drop(columns=[\"RowID\", \"Distillery\", \"Postcode\", \" Latitude\", \" Longitude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a7a054",
   "metadata": {},
   "source": [
    "Let's take a look at the flavour indicators and see how they might be related to one another. \n",
    "\n",
    "**Assignment 2.1:** Compute the correlation matrix for the flavour indicators using NumPy's `np.corrcoef` function `correlation_matrix`.\n",
    "> *Hint*: unlike the notation in the lecture notes `np.corrcoef` expects attributes as rows, so make sure to set `rowvar=False` or transpose the data matrix `X` using e.g. `X.T` before inputting to `np.corrcoef`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27dceea0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed8e9de9416dcff6f630f23af2795980",
     "grade": false,
     "grade_id": "cell-e48c7598076c84bd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Assignment 2.1: Compute the correlation matrix of the dataset using numpy, save it as correlation_matrix.\n",
    "correlation_matrix = np.corrcoef(X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dab8e9b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8407d34970212dd3c2cec2d9b54a7657",
     "grade": true,
     "grade_id": "cell-a760d25e3fe3de78",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"\n",
       "            padding:10px 12px;\n",
       "            margin:8px 0;\n",
       "            border:1px solid #c3e6cb;\n",
       "            background:#d4edda;\n",
       "            color:#155724;\n",
       "            border-radius:6px;\n",
       "            font-family:sans-serif;\n",
       "        \">\n",
       "            <div style=\"margin:1px; font-weight:600;\"> [1 marks]</div>\n",
       "            \n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visible test\n",
    "with utils.marks(1):\n",
    "    assert(utils.check_hash(correlation_matrix, ((12,12), 962.9959327561705)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5530ecce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the correlation matrix\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "fig.suptitle('Correlation matrix', fontsize=16)\n",
    "# Plot correlation matrix and set colorbar min and max to -1 and 1 since thats the range of the correlation coefficients\n",
    "plt.imshow(correlation_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.xticks(ticks=np.arange(correlation_matrix.shape[1]), labels=list(X.columns), rotation=90)\n",
    "plt.yticks(ticks=np.arange(correlation_matrix.shape[1]), labels=list(X.columns))\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba8c0a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9da3be50e7cf360ffc6ea9d03125d7f",
     "grade": false,
     "grade_id": "cell-049d7d7e53db4a31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Assignment 2.2:** Which flavour indicators are most positively correlated? And negatively correlated? <br> Do these flavour correlations make sense to you? Why?\n",
    "- *Answer (max 200 words):* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297641c7-b933-4f1f-bfeb-18bf02ed2c7e",
   "metadata": {},
   "source": [
    "[your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ef158-e34c-4f2b-a1f6-b86e0b29e6f8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58a4c70c01c20e8947d33244e36baf9c",
     "grade": true,
     "grade_id": "cell-3721125ea6cca70f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "891fecca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7dcb1c98b64adad666f7ea10f5b65ab3",
     "grade": true,
     "grade_id": "cell-f0d39544feb630a5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"\n",
       "            padding:10px 12px;\n",
       "            margin:8px 0;\n",
       "            border:1px solid #ffeeba;\n",
       "            background:#fff3cd;\n",
       "            color:#856404;\n",
       "            border-radius:6px;\n",
       "            font-family:sans-serif;\n",
       "        \">\n",
       "            <div style=\"margin:1px; font-weight:600;\">? [manual assessment: 1 marks]</div>\n",
       "            \n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual test\n"
     ]
    }
   ],
   "source": [
    "with utils.marks(1, auto=False, visible=False):\n",
    "    print(\"Manual test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1c23c0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "While we can look at how the individual flavour indicators relate to each other, it is also interesting to look at how the distilleries are related to each other in terms of their flavour profiles. We can compute distances in **flavour space** between distilleries.\n",
    "\n",
    "For example, we can compute the distance between the `Aberfeldy` distillery and every other distillery *in terms of flavour, not physical distance*. The result will be a 1D array of 86 distances. \n",
    "\n",
    "As you have seen in the lecture, a common distance measure is the **general Minkowski distance** also known as the **$p$-norm** or **$p$-distance**:\n",
    "$$\n",
    "    d_p\\left(\\boldsymbol{x}^\\ast, \\boldsymbol{x}\\right) = \\left( \\sum_{j=1}^M \\lvert x^\\ast_j - x_{j}\\rvert^p\\right)^{\\frac{1}{p}}\n",
    "$$\n",
    "which reduces to the cityblock (or Manhattan) distance for $p=1$ and the Euclidean distances for $p=2$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{Cityblock}: \\ d_1\\left(\\boldsymbol{x}^\\ast, \\boldsymbol{x}\\right) = \\sum_{j=1}^M \\lvert x^\\ast_j - x_{j}\\rvert \\qquad \\qquad\n",
    "    \\text{Euclidean}: \\ d_2\\left(\\boldsymbol{x}^\\ast, \\boldsymbol{x}\\right) = \\sqrt{\\sum_{j=1}^M \\lvert x^\\ast_j - x_{j}\\rvert^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Assignment 2.3:** Complete the p-norm distance function in the cell below using basic numpy functions `np.sum()`and `np.abs()`\n",
    "> *Hint:* Remember to think of which axis the sum should be calculated over - such that we get an array of distances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d575402",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56e7eb3c36dd1adc2863b94e45ba48d8",
     "grade": false,
     "grade_id": "cell-8bf109a1e0cfc6c2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Assignment 2.3: Implement the p-norm distance function (this is key in many machine learning algorithms, so make sure you understand this!)\n",
    "def p_norm(X_star, X, p = 2):\n",
    "    \"\"\"\n",
    "    Compute the p-norm distance between X_star and each row in X using numpy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_star : np.ndarray. A 1D array of shape (M,) or a 2D array broadcastable to X.\n",
    "    X : np.ndarray A 2D array of shape (N, M), where each row is a data point.\n",
    "    p : int or float, optional. The order of the norm (default is 2).\n",
    "\n",
    "    Returns: \n",
    "    np.ndarray. A 1D array of shape (N,) containing the p-norm distances.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f4001",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03119a0a14a9f2200864ffb6f06d93ee",
     "grade": true,
     "grade_id": "cell-59fbb60f97264a4b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with utils.marks(1, auto=False, visible=False):\n",
    "    print(\"Manual test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5565a6",
   "metadata": {},
   "source": [
    "To test the implementation, we will now use the p-norm function to calculate the cityblock and euclidean distance in **flavour space**.\n",
    "\n",
    "**Assignment 2.4:** Calculate the cityblock and euclidean distance between the Aberfeldy distillery and all of the other distilleries in the dataset - using the p_norm function. Save the results in the variables `distances_p1` and `distances_p2`.\n",
    "> *Hint:* You can use the `iloc` function to select a row from the dataframe - which row is the Aberfeldy distillery in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c681a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2fb9dc72b781ec0786bca2d27c5b4a00",
     "grade": false,
     "grade_id": "cell-8ff5b53d7de5906b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Assignment 2.4: Calculate distances p=1 and p=2 between the Aberfeldy and all distilleries (including itself)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c43fd8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7767acb98b035c52262e7ebe4af72cd1",
     "grade": true,
     "grade_id": "cell-16c02a851045eada",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible test\n",
    "with utils.marks(2):\n",
    "    assert(utils.check_hash(distances_p1, ((86,), 28428.568570846488)))\n",
    "    assert(utils.check_hash(distances_p2, ((86,), 11873.005164282675)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4739264",
   "metadata": {},
   "source": [
    "**Assignment 2.5:** You had to implement the Cosine Similarity function in **Task 1.5** today, use it to calculate the cosine similarity between **Aberfeldy** and all whisky distilleries in the dataset - save the result in a variable called `cos_sim`.\n",
    "> *Hint:* If you did not implement the Cosine Similarity function, you can find it in the solution of **Task 1.5**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5182bf4b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "112e74d20dfa3d9a743952c45195c5c7",
     "grade": false,
     "grade_id": "cell-235cf25ec5acbb02",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Assignment 2.5: Compute the cosine similarity between Aberfeldy and all whisky distilleries, save the result in a variable called cos_sim.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf475f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f57f5703e2dc56ae0284e69e93ae9fb3",
     "grade": true,
     "grade_id": "cell-d44e708d1d25c4d4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible test\n",
    "with utils.marks(1):\n",
    "    assert(utils.check_hash(cos_sim, ((86,), 3221.3814891360826)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e2326",
   "metadata": {},
   "source": [
    "Now that we have computed some similarities and dissimilarities, we can create a sorted plot of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56141e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8), dpi=150)\n",
    "\n",
    "def rank_plot(distances, y_label = \"Distance\"):\n",
    "    # find the ordering of the distances\n",
    "    order = np.argsort(distances)\n",
    "    # bar plot them\n",
    "    ax.bar(np.arange(len(distances)), distances[order])\n",
    "    ax.set_ylabel(y_label, fontsize=12)\n",
    "    ax.set_xticks(np.arange(86))\n",
    "    ax.set_frame_on(False)\n",
    "    # make sure the same order is used for the labels!\n",
    "    ax.set_xticklabels(\n",
    "        np.array(distilleries[np.argsort(distances)]), rotation=\"vertical\", fontsize=8\n",
    "    )\n",
    "\n",
    "\n",
    "# make the plots\n",
    "ax = fig.add_subplot(3, 1, 1)\n",
    "ax.set_title(\"Manhattan/cityblock/L1 ($p=1$)\", fontsize=16)\n",
    "rank_plot(distances_p1)\n",
    "ax = fig.add_subplot(3, 1, 2)\n",
    "ax.set_title(\"Euclidean/L2 ($p=2$)\", fontsize=16)\n",
    "rank_plot(distances_p2)\n",
    "ax = fig.add_subplot(3, 1, 3)\n",
    "ax.set_title(\"Cosine Similarity\", fontsize=16)\n",
    "rank_plot(cos_sim, y_label=\"Cosine similarity\")\n",
    "\n",
    "# removes ugly overlapping\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc4b473",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Assignment 2.6:** Imagine you work in a whisky store. A customer tells you they love whiskies from **Aberfeldy**. Which whiskies would you recommend them to purchase, and which would you not recommend? Why? \n",
    "\n",
    "- *Answer (max 200 words):*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e738108-5dc5-40a2-b7d3-53d4865db054",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1ad35433db95019c145998fc915f06f",
     "grade": true,
     "grade_id": "cell-0a97f95ae057ab61",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad8744",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08cc93a460fa2c7e08d3f5ce41046961",
     "grade": true,
     "grade_id": "cell-99213ce122aa0f9d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with utils.marks(1, auto=False, visible=False):\n",
    "    print(\"Manual test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e4169c-a279-4723-a025-da0272a9d6fe",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Different geographic regions are associated with distinct flavour profiles. One of the most distinctive is the island of Islay, shown below. Owing to \n",
    "the extensive use of peat in the manufacturing process, whiskies from Islay are renowned for their bold, smoky, and unmistakable character."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28084dc5-bfd5-4bb1-8f75-ca6dc457fa97",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <img src=\"data/map.png\" width=\"70%\">\n",
    "</center"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf5083-63a6-4b48-b1d2-a4cbf722c4ce",
   "metadata": {},
   "source": [
    "We create a new boolean target variable, `y`, that tells us whether the distillery is from a **Islay**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a2a60d-7646-4771-958e-5595c6852e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "islay_whiskies = [\n",
    "    \"Ardbeg\",\n",
    "    \"Bowmore\",\n",
    "    \"Bruichladdich\",\n",
    "    \"Bunnahabhain\",\n",
    "    \"Caol Ila\",\n",
    "    \"Lagavulin\",\n",
    "    \"Laphroaig\"\n",
    "]\n",
    "\n",
    "# Boolean indicator: True if distillery is from Islay, else False\n",
    "y = [d in islay_whiskies for d in distilleries]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62576af9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    " A new distillery opens up, and an expert whisky drinker judges the new distillery on the flavour indicators.\n",
    "They judge the whisky with the following scores: \n",
    "\n",
    "| Body | Sweetness | Smoky | Medicinal | Tobacco | Honey | Spicy | Winey | Nutty | Malty | Fruity | Floral |\n",
    "|------|-----------|-------|-----------|---------|-------|-------|-------|-------|-------|--------|--------|\n",
    "| 4.0  | 1.0       | 4.0   | 3.0       | 1.0     | 0.0   | 2.0   | 0.0   | 1.0   | 1.0   | 1.0    | 0.0    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c3d85",
   "metadata": {},
   "source": [
    "**Assignment 2.7:** Create a new one-row pandas DataFrame, `X_new, representing the flavour profile of a new distillery, using the same attributes and column names\n",
    "\n",
    "> *Hint:* Take inspiration from **Task 3.9**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168784c0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd7df81e44bca8b29e456b33e16cdff0",
     "grade": false,
     "grade_id": "cell-338dc8cd6dd48f5c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Assignment 2.7: Create a new data object, X_new, with the attributes given in the text.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Print the result\n",
    "X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd22e7e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a5c3c2798c855d50b1fa4713068035c",
     "grade": true,
     "grade_id": "cell-f5abaf7e281f6b71",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible test\n",
    "with utils.marks(1):\n",
    "    assert(utils.check_hash(X_new, ((1, 12), 86.88443731048635)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed5c7ee",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "**Assignment 2.8:** Initialize and train a k-nearest neighbours (kNN) classification model with $K=5$, then use it to predict whether the new distillery (`X_new`) is from Islay. Save the result as a NumPy array called `y_pred` containing a boolean value.\n",
    "> *Hint:* Take inspiration from **Tasks 3.7 - 3.10**. \n",
    "\n",
    "> *Hint:* You do **not** need to standardize the data, as all flavour variables are on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd641b6d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4df7ce0d517c8c51a6676775fea341ac",
     "grade": false,
     "grade_id": "cell-8ba20f55bc99815b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Assignment 2.8: Initialise, fit and predict using the KNN classifier - save the predicted label as y_pred\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cdb200",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0248feb11883122e8b1b0ed5b539c284",
     "grade": true,
     "grade_id": "cell-949dfc731e97e05e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible test\n",
    "with utils.marks(1, auto=True, visible=True):\n",
    "    assert(utils.check_hash(y_pred, ((1, ), 5.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f146f825-6413-4d07-a7db-8e9e76912ffc",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cff71af",
   "metadata": {},
   "source": [
    "**Assignment 2.9:** We now want to find the 5 nearest distillieries in **Flavour Space**, use your model to find the indices and save their them in `closest_neighbors_indices`. \n",
    "\n",
    "> *Hint:* You had a very similar task in **Task 2.4**, if you had difficulty with the task, you can look at the solution file. \n",
    "\n",
    "> *Hint:* Once you find the indices of the closest neighbours, you can index `distilleries`, which contained the names of distilleries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cdb6c0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f6f0d6572ab434b22cf0cdf293376a3",
     "grade": false,
     "grade_id": "cell-2349e2b412e62049",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Assignment 2.9: Find the indices of the closest neighbours to the new point and save them in \"closest_neighbors_indices\".\n",
    "\n",
    "# Get the indices of the closest neighbors from the training set and save in closest_neighbors_indices\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Get the closest neighbours and their names\n",
    "closest_distilleries = distilleries[closest_neighbors_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa205ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86ca491c8130eb0614fe112b3a78c8e2",
     "grade": true,
     "grade_id": "cell-e5b746c105c5b579",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# A test for checking the list of closest_distilleries\n",
    "display(closest_distilleries)\n",
    "\n",
    "with utils.marks(1, auto=True, visible=False):\n",
    "    print(\"Hidden test\")\n",
    "    ### HIDDEN TESTS \n",
    "    # Result should be: Ardbeg, Clynelish, Caol Ila, Lagavulin, Talisker\n",
    "    print(\"Hash of closest_distilleries:\", utils._sha(sorted(list(closest_distilleries))))\n",
    "    assert utils._sha(sorted(list(closest_distilleries)))=='c6c679f70ed53ca857dac265d5bc0c1c9f8b1977d9c2710854bf31deb955bc18',\"List not correct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de04808c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Assignment 2.10:** Declaration on the use of AI for solving this assignment (mandatory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c7d77b",
   "metadata": {},
   "source": [
    "Answer by replacing *[your answer here]* with your response.\n",
    "1. *I abide by [DTU's code of honour](https://student.dtu.dk/en/exam/exam-cheating/dtu-code-of-honour) and take full responsibility for the content of this submission (yes / no)*: \n",
    "    - [your answer here] \n",
    "2. *To what extent did you use generative AI to solve this assignment (0-100%)*:\n",
    "    - [your answer here] \n",
    "3. *What was the primary use of generative AI, if any (writing / coding / other)*: \n",
    "    - [your answer here] \n",
    "4. *I feel I understand the key techniques/algorithms/methods/coding elements used in this exercise such that I can apply it in a **no aids** exam (yes/no)*: \n",
    "    - [your answer here] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb13a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6ea0eb3a6fff40d95e23a6d7c83a85f",
     "grade": true,
     "grade_id": "cell-36fe15b737e44abe",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with utils.marks(0, auto=False, visible=False):\n",
    "    print(\"Manual test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730a3cca",
   "metadata": {},
   "source": [
    "**Assignment 2.11:** To submit this notebook, make sure you have run everything in the **Assignment**-part, and convert this notebook to an HTML file.\n",
    "\n",
    "> *Hint:* Open the **command palette** in Visual Studio Code, by pressing `Cmd + Shift + P` on Mac or `Ctrl + Shift + P` on Windows. \n",
    "\n",
    "> *Hint:* Search for `Jupyter: Export to HTML` and save the HTML file. \n",
    "\n",
    "> *Hint:* If you are running the notebook in the browser via the Jupyter interface, go to `File`, then `Save` and choose `Save and Export Notebook` as and select HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5eccdb",
   "metadata": {},
   "source": [
    "**Assignment 2.12:** Hand in your `.ipynb` and `.HTML` file under assignments on DTU Learn for the relevant week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612398f5",
   "metadata": {},
   "source": [
    "**<h4>Summary of the points (only valid after rerunning the Assignment-part from scratch)</h4>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4714af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.marks_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
